\documentclass[a4paper]{article} 
\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Ryan Ott \hfill\\   
14862565 \hfill\\
ryan.ott@student.uva.nl
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Practical Assignment 1\\ 
\normalsize 
Deep Learning 1\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------
\section{Linear Module and Activation Module}
\subsection{Linear Module} %a
\begin{align}
   \left[ \frac{\partial L}{\partial \mb{W}} \right]_{ij} &= \frac{\partial L}{\partial W_{ij}} = \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial W_{ij}} \\
   \frac{\partial Y_{sn}}{\partial W_{ij}} &= \frac{\partial}{\partial W_{ij}} \left( \sum_{m} [\mb{X}]_{sm} [\mb{W}^\top]_{mn} + [\mb{B}]_{sn} \right) = \sum_m X_{sm} \frac{\partial W_{nm}}{\partial W_{ij}} + \frac{\partial B_{sn}}{\partial W_{ij}} \\
   &= \sum_m X_{sm} \delta_{ni} \delta_{mj} + 0 = X_{sj} \delta_{ni} \\
   \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial W_{ij}} &= \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} X_{sj} \delta_{ni} = \sum_{s} \frac{\partial L}{\partial Y_{si}} X_{sj} \\
   \therefore \frac{\partial L}{\partial \mb{W}} &= \left( \frac{\partial L}{\partial \mb{Y}} \right)^\top \mb{X} \quad \in \mathbb{R}^{N \times M}
\end{align}
\bigskip
% --------------------------------

\subsection{} %b
\begin{align}
   \left[ \frac{\partial L}{\partial \mb{b}} \right]_{j} &= \frac{\partial L}{\partial b_{j}} = \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial b_{j}} \\
   \frac{\partial Y_{sn}}{\partial b_{j}} &= \frac{\partial}{\partial b_{j}} \left( \sum_{m} [\mb{X}]_{sm} [\mb{W}^\top]_{mn} + [\mb{B}]_{sn} \right) = \sum_m \frac{\partial X_{sm} W_{nm}}{\partial b_{j}} + \frac{\partial B_{sn}}{\partial b_{j}} \\
   &= 0 + \delta_{nj} = \delta_{nj} \\
   \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial b_{j}} &= \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \delta_{nj} = \sum_{s} \frac{\partial L}{\partial Y_{sj}} \\
   \therefore \frac{\partial L}{\partial \mb{b}} &= \sum_{s} \frac{\partial L}{\partial \mb{Y}_s} \quad \in \mathbb{R}^{1 \times N}
\end{align}
For clarification, in equation (10) we sum over the rows $s \in S$ of $\mb{Y}$, so the $j$-th element of $\mb{b}$ is the sum of all elements in position $j$ of the rows of $\mb{Y}$.
\bigskip
% --------------------------------

\subsection{} %c
\begin{align}
   \left[ \frac{\partial L}{\partial \mb{X}} \right]_{ij} &= \frac{\partial L}{\partial X_{ij}} = \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial X_{ij}} \\
   \frac{\partial Y_{sn}}{\partial X_{ij}} &= \frac{\partial}{\partial X_{ij}} \left( \sum_{m} [\mb{X}]_{sm} [\mb{W}^\top]_{mn} + [\mb{B}]_{sn} \right) = \sum_m \frac{\partial X_{sm}}{\partial X_{ij}} W_{nm} + \frac{\partial B_{sn}}{\partial X_{ij}} \\
   &= \sum_m \delta_{si} \delta_{mj} W_{nm} + 0 = \delta_{si} W_{nj} \\
   \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \frac{\partial Y_{sn}}{\partial X_{ij}} &= \sum_{s,n} \frac{\partial L}{\partial Y_{sn}} \delta_{si} W_{nj} = \sum_{n} \frac{\partial L}{\partial Y_{in}} W_{nj} \\
   \therefore \frac{\partial L}{\partial \mb{X}} &= \frac{\partial L}{\partial \mb{Y}} \mb{W} \quad \in \mathbb{R}^{S \times M}
\end{align}
\bigskip
% --------------------------------

\subsection{Activation Module} %d
\begin{align}
   \left[ \frac{\partial L}{\partial \mb{X}} \right]_{ij} &= \frac{\partial L}{\partial X_{ij}} = \sum_{s,m} \frac{\partial L}{\partial Y_{sm}} \frac{\partial Y_{sm}}{\partial X_{ij}} \\
   &= \sum_{s,m} \frac{\partial L}{\partial Y_{sm}} \frac{\partial h(X_{sm})}{\partial X_{ij}} \\
   &= \sum_{s,m} \frac{\partial L}{\partial Y_{sm}} \delta_{si} \delta_{mj} = \frac{\partial L}{\partial Y_{ij}} \\
   \therefore \frac{\partial L}{\partial \mb{X}} &= \frac{\partial L}{\partial \mb{Y}} \circ h'(\mb{X}) \quad \in \mathbb{R}^{S \times M}
\end{align}
From equation (17) to (18) we get two Kronecker deltas because the derivative of the element-wise activation function
is zero for all elements except the one we are taking the derivative of. This makes intuitive sense, given we are
differentiating an element-wise function. The final derivative of the loss w.r.t. the input $\mb{X}$ is the Hadamard
product of the derivative of the loss w.r.t. the output $\mb{Y}$ and the element-wise derivative of the activation
function $h$ w.r.t. the input $\mb{X}$. We can assume the shapes of $\mb{X}$ and $\mb{Y}$ are compatible, since $\mb{Y}=h(\mb{X})$
\bigskip
% ==================================

\section{Softmax, Loss Modules and Residuals}

\subsection{Softmax \& Loss Modules} %a
\bigskip
% --------------------------------

\subsection{Residuals} %b
\textit{Which constraints does the residual connection place on $N_1$ and $N_1$, the numbers of neurons in the two linear layers of the LAL module?}\\
The first linear layer $L_1$ must have have $F$ rows to allow for the matrix multiplication with the input $\mb{X} \in \mathbb{R}^{S \times F}$.
It will then have an arbitrary output dimension (that is compatible with the activation function). The second linear layer $L_2$ must however have
the exact same shape as $X$ to allow for the element-wise addition of the residual connection. So $N_2 = S \times F$.
\bigskip
% --------------------------------

\subsection{} %c
\textit{How does adding the residual connection change $\frac{\partial L}{\partial \mb{X}}$?}\\
The derivative of the loss now has an additional term, which is the derivative of the residual connection. This is the identity function,
so the derivative is 1. The derivative of the loss w.r.t. the input $\mb{X}$ is now the sum of the derivative of the loss w.r.t. the output $\mb{Y}$
plus the derivative of the residual connection w.r.t. the input $\mb{X}$.
\begin{align}
   \frac{\partial L}{\partial \mb{X}} &= \frac{\partial L}{\partial \mb{Y}}\mb{W} + \frac{\partial \mb{X}}{\partial \mb{X}} \\
   &= \frac{\partial L}{\partial \mb{Y}}\mb{W} + \mb{I}
\end{align}

\bigskip
% --------------------------------

\subsection{} %d
\textit{Briefly explain how your answer to (c) improves the stability of training a deep neural network made up of many
such residual blocks, also known as ResNet.}
\begin{itemize}
   \item \textbf{Vanishing gradients:} By adding residual connections, the gradient of the loss w.r.t. the input $\mb{X}$
   can take an alternative path through the network and is far less likely to vanish. Applying the chain rule many times
   makes us multiply small numbers by each other, making the gradient smaller as it propagates. By adding the input
   element-wise, we keep an alternative, non-vanishing path in the backward.
   \item \textbf{Reduced Error Propagation:} The network is able to learn more complex functions and isn't limited to passing
   on the errornous output of the previous layer. It can learn more low-level features from the input or highly process them
   to learn high-level features.
   \item \textbf{Smoother Loss Landscape:} Because the gradients dont vanish as easily, there are less sharp edges in the loss
   surface. Also, by allowing both high and low-level features to be passed on, the problem can be modeled better, thus smoothing
   the loss surface.
\end{itemize}
\bigskip
% ==================================

\section{NumPy Implementation}
Test accuracy was 49.88\% using the default settings.
We see that the training loss is monotonically decreasing, which is what we hope for. The network is learning something with each
iteration. When looking at validation accuracy over the epochs, we see that it is increasing, but not monotonically. This is because
the network is generally getting better at modelling the data, but sometimes it might be over or under fitting, thus performing worse
on unseen data.
% ==================================

\section{PyTorch Implementation}
Test accuracy was 47.89\% using the default settings.
Very similar to the NumPy implementation, with a monotonically decreasing training loss and a validation accuracy curve that increases
but not monotonically. Noteworthy is that the network seems to make a bigger jump in terms of training loss improvment after the first
epoch and that it seems to over/underfit more, as the validation accuracy fluctuates and dips more considerably. This is further
made evident by the slightly lower test accuracy.
% ==================================
\bigskip

\section{Optimization}
\subsection{Hessian Matrix at Local Minimum having Positive Eigenvalues}
\textit{Show that the Hessian matrix of a function $f(\mb{x})$ at a local minimum has only positive eigenvalues.}\\
From the assignment description we know that if $H$ at point pp is positive definite, then we have are at a strictly
local minimum. To prove that the eigenvalues $\lambda$ of $H$ at a local minimum are positive, we must show that
the inequality from Lecture 3 slide 31 holds only for positive eigenvalues:
\begin{align}
   \mb{x}^{\top} H \mb{x} > 0 \quad \forall \mb{x} \in \mathbb{R}^n \setminus \{0\}
\end{align}

Suppose that $H$ has an eigenvalue $\lambda$, we can then write the eigenvalue decomposition of $H$ as:
\begin{align}
   H\mb{x} = \lambda \mb{x}
\end{align}

If $\lambda=0$, then there is some eigenvector $\mb{x}$ such that $H\mb{x}=0$. But then the inequality (21) would no
longer hold because $\mb{x}^{\top} H \mb{x} = 0$. So $H$ would not be positive definite if it had an eigenvalue of 0
and by extension we would not be at a local minimum.

If $\lambda<0$, then there is some eigenvector $\mb{x}$ such that $H\mb{x}=\lambda \mb{x}$.
If we examine this further, we get:
\begin{align}
   \mb{x}^{\top} H \mb{x} &= \mb{x}^{\top} \lambda \mb{x} \\
   \implies \mb{x}^{\top} H \mb{x} &= \lambda \mb{x}^{\top} \mb{x} \\
   \implies \mb{x}^{\top} H \mb{x} &= \lambda |\mb{x}|^2
\end{align}

We see that the inequality (21) would no longer hold because $\mb{x}^{\top} H \mb{x} = \lambda |\mb{x}|^2 < 0$ due to
$\lambda<0$. So $H$ would not be positive definite if it had a negative eigenvalue and by extension we would not be at
a local minimum.

But then $\mb{x}^{\top} H \mb{x}=\lambda \mb{x}^{\top} \mb{x}$,
which is negative since $\mb{x}^{\top} \mb{x}>0$ and $\lambda<0$. Thus $H$ is not positive definite.

And so for $H$ at a local minimum, all eigenvalues must be positive.
\bigskip
% --------------------------------

\subsection{Exponentially Higher Number of Saddle Points than Local Minima}
For a location to be a local minimum, all eigenvalues must be positive as proven in the previous question. For a
location to be a saddle point however, there must be at least one negative and one postive eigenvalue.
If we take the sign of an eigenvalue to be a binary variable, we can see that there are $2^n$ possible combinations
of eigenvalue signs for $n$ dimensions. Out of all combinations, only one is a local minimum, while almost all
remaining combinations are saddle points (except for the one where all eigenvalues are negative, which would be a
local maximum). As dimensionality increases, it becomes exponentially more likley that not all eigenvalues will be
positive. So the number of saddle points is exponentially higher than the number of local minima.
\bigskip
% --------------------------------

\subsection{Gradient Descent around a Saddle Point}
Firstly, because as the assignment description states, at a saddle point p the gradient is zero
$\nabla_{\mb{x}} f(\mb{x}_p) = 0$, when we look at the gradient descent update formula we see that the update
will be zero as well:
\begin{align}
   \mb{w}_{t+1} &= \mb{w}_t - \eta \cdot \nabla_{\mb{w}} f(\mb{w}_t) \\
   \implies \mb{w}_{t+1} &= \mb{w}_t - \eta \cdot 0 \\
   \implies \mb{w}_{t+1} &= \mb{w}_t
\end{align}

So in the close vicinity of a saddle point, the gradient descent update will be minimally small, harming the
effectiveness of the learning.

Furthermore, when looking at Lecture 3 slide 34, we can see using the 2$^{\text{nd}}$ order Taylor expansion that
after an update the new loss can be approximated as:
\begin{align}
   \mathcal{L}(w' - \varepsilon g) \approx \mathcal{L}(w') - \varepsilon g^Tg + \frac{1}{2} \varepsilon^2 g^THg
\end{align}

Where $g^Tg$ (the first order term) represents the magnitude of the gradient, and $g^THg$ (second order term)
represents the curvature in the direction of the gradient.
If $\frac{1}{2} \varepsilon^2 g^THg > \varepsilon g^Tg$, the update step can increase the loss because the effect
of the curvature (the second-order term) is greater than the effect of the gradient (the first-order term).
\bigskip
% ==================================

\section{Precision, Recall, Accuracy and F1-beta Score}
\subsection{When to use which metric}
Precision: Used when the cost of false positives is high. For example, in email spam detection (labeling a
legitimate email as spam is worse than missing a spam email) and in legal systems (labeling an innocent person
as guilty is highly undesirable).

Recall: Used when missing true positives is costly. For example, in fraud detection systems (missing a fraudulent
transaction can be very costly, rather one would do an unnecessary double check) and in medical diagnosis (missing
a disease can be fatal).

Accuracy: Used when the costs of false positives and false negatives are relatively balanced and the class
distribution is even. For example, in a balanced binary classification problem like sentiment analysis
(positive vs negative reviews) and in gender classification based on images.
\bigskip
% --------------------------------

\subsection{Confusion Matrix}
Using NumPy, we get the following confusion matrix for the CIFAR10 dataset:

\bigskip
% --------------------------------

\subsection{F1-beta Score}
Using NumPy, we get the following F1-beta scores for the CIFAR10 dataset:

\end{document}