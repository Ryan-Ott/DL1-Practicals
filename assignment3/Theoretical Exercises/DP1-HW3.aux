\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Generative Part of the VAE}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Steps needed to sample an image from the decoder}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Why Monte-Carlo sampling is inefficient for VAE training}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Why ELBO is a lower bound on the log-likelihood}{1}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}What happens to left hand terms when when the lower bound is pushed up}{1}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Why are the names reconstruction loss and regularization loss appropriate}{1}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Why does sampling prevent gradient computation and how does reparameterization solve this}{1}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}VAE Training, Validation \& Testing bpd}{2}{subsection.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training, validation and testing bpd for the VAE\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:VAE_training}{{1}{2}{Training, validation and testing bpd for the VAE\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Samples During Training}{2}{subsection.1.8}\protected@file@percent }
\newlabel{fig:epoch0}{{2a}{2}{Before Training\relax }{figure.caption.2}{}}
\newlabel{sub@fig:epoch0}{{a}{2}{Before Training\relax }{figure.caption.2}{}}
\newlabel{fig:epoch10}{{2b}{2}{After 10 Epochs\relax }{figure.caption.2}{}}
\newlabel{sub@fig:epoch10}{{b}{2}{After 10 Epochs\relax }{figure.caption.2}{}}
\newlabel{fig:epoch80}{{2c}{2}{After 80 Epochs\relax }{figure.caption.2}{}}
\newlabel{sub@fig:epoch80}{{c}{2}{After 80 Epochs\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sample Quality Across Training Epochs\relax }}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:three_epochs}{{2}{2}{Sample Quality Across Training Epochs\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Manifold}{2}{subsection.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Adverserial Autoencoder Networks}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}How to compute $q(\mathbf  {z})$ based on different encoders}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.a}$q(\mathbf  {z})$ being a dirac delta function}{3}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.b}$q(\mathbf  {z})$ being a Gaussian distribution}{3}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.c}$q(\mathbf  {z})$ being a universal approximator}{3}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}How do Adversarial Autoencoders reduce the mode collapse problem compared to vanilla GANs}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Adversarial Autoencoder Training Objective}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.a}Three Terms of the Formula}{4}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.b}Effect of $\lambda $ on the Training Objective}{4}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Reconstruction Phase}{4}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularisation Phase}{4}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Simple Autoencoder vs Adversarial Autoencoder}{4}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Stochastic vs Deterministic Encoder}{4}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Transformers}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Challenge of Long Input Sequence Lengths}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Causal Self-Attention}{5}{subsection.3.2}\protected@file@percent }
\newlabel{lst:causal-self-attention}{{1}{5}{Causal Self-Attention Forward}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces Causal Self-Attention Forward}}{5}{lstlisting.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Generate Function}{5}{subsection.3.3}\protected@file@percent }
\newlabel{lst:generate}{{2}{5}{GPT Class Generate Function}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}{\ignorespaces GPT Class Generate Function}}{5}{lstlisting.2}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\gdef \@abspage@last{5}
